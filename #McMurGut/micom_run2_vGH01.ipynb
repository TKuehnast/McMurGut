{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168184f7-70f4-4ad9-a4ee-d54af03e0439",
   "metadata": {},
   "source": [
    "# Script Micom run2\n",
    "Version GitHub 01\n",
    "\n",
    "**Running Micom**\n",
    "\n",
    "Micom v0.35.0  \n",
    "qiime2-amplicon-2024.2  \n",
    "\n",
    "########################################################  \n",
    "By Torben Kuehnast, torben.kuehnast@gmail.com, 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc21b6-4ffe-4548-8ccc-818e1a510606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import contextlib\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from fnmatch import fnmatch\n",
    "from datetime import datetime\n",
    "\n",
    "from micom import show_versions\n",
    "from micom.workflows import build\n",
    "from micom.workflows import tradeoff\n",
    "from micom.workflows import grow\n",
    "from micom.stats import compare_groups\n",
    "from micom.measures import production_rates\n",
    "from micom.viz import plot_tradeoff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ae2ed-501c-4a62-93f2-a9374a85a1aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "# Insert files\n",
    "#############################################################################################\n",
    "\n",
    "# ---> INSERT project name, a unique name which will be prefixed to all your output files\n",
    "project = 'project_name'\n",
    "\n",
    "\n",
    "# ---> INSERT current running version, also prefixed to all your output files\n",
    "version_nr = 'version'\n",
    "\n",
    "pro_ver = project+\"_\"+version_nr\n",
    "\n",
    "\n",
    "\n",
    "# ---> INSERT the basic directory\n",
    "# Herein, data can be provided and accessed\n",
    "# Also, a new folder will be created based on your project name and version, where all output files will be stored (working directory)\n",
    "\n",
    "basic_dir = '/home/basic_directory'\n",
    "\n",
    "working_dir = os.path.join(basic_dir, pro_ver)\n",
    "\n",
    "if not os.path.exists(working_dir):\n",
    "    os.makedirs(working_dir)\n",
    "    print(f\"Folder {working_dir} was created.\")\n",
    "else:\n",
    "    print(f\"Folder {working_dir} already exists.\")\n",
    "print(\"Output folder:\", working_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---> INSERT TAS/taxonomy and abundance from all 8 samples\n",
    "\n",
    "# Important information:\n",
    "\n",
    "# The columns of this CSV file need to contain the following headers:\n",
    "# kingdom;phylum;class;order;family;genus;species;sample_id;abundance\n",
    "# All other column will not be taken into account\n",
    "\n",
    "filepath_crop_fmtas = '/home/project_final_micom_tas.csv'\n",
    "data = pd.read_csv(filepath_crop_fmtas, sep=\",\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----> INSERT 2 conditions/day 1 vs day 13 in cachexia\n",
    "\n",
    "# Important information:\n",
    "\n",
    "# The columns need to be as follows:\n",
    "# \"sample;condition\"\n",
    "# For each sample, you provide the condition's name, like\n",
    "# TK_137;day01_healthy\n",
    "# TK_033;day13_sick\n",
    "# Add all samples you have with the respective conditions\n",
    "# It will be used to group your samples according to the conditions, to later on look at them differentially\n",
    "\n",
    "filepath_condition = '/home/condition_file.csv'\n",
    "data_condition = pd.read_csv(filepath_condition, sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189f9b6-f0c8-4920-a5e4-e005e179ea36",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----> INSERT CPU power used. 1 sample = 1 CPU\n",
    "cpus_used = int(39)\n",
    "\n",
    "\n",
    "\n",
    "# ----> INSERT Model catalog. Like McMurGut 1.1\n",
    "\n",
    "# Important information:\n",
    "# Make sure the metabolic IDs are the same in the 1) model catalog and 2) the diet file!\n",
    "# McMurGut is based on gapseq, which is based on the ModelSEED nomenclature, like metabolite-id = \"cpd00001\" for water.\n",
    "\n",
    "test_db = '/home/MCMG754_genus.qza'\n",
    "\n",
    "\n",
    "\n",
    "# ---> INSERT the medium you are using\n",
    "# Herein, the medium was selfmade, based on the ssniff-diet and completed with Micoms complete_db_medium() function\n",
    "# based on McMurGut 1.1\n",
    "\n",
    "test_medium = '/home/ssniff_MCMG754_v04_diet.qza'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---> INSERT the full list of all metabolites in THEORY available to gapseq models\n",
    "# This is needed to transform cpd00001 into more meaningful metabolic IDs (--> H2O) at the end of the script\n",
    "# and to provide additional information in the middle of the script. Example:\n",
    "# \"id;name\"\n",
    "# \"cpd00001;H2O_MNXM2\"\n",
    "# \"cpd00002;ATP_MNXM3\"\n",
    "# \"cpd00003;NAD_MNXM8\"\n",
    "# It has to contain the \"id\" and \"name\" header.\n",
    "\n",
    "filepath_allmetab = '/home/all_gapseq_metabolites_756_compgr.csv'\n",
    "allmetab = pd.read_csv(filepath_allmetab, sep=\";\")\n",
    "\n",
    "\n",
    "# ---> INSERT cutoff value of abundance for taxa which will no longer taken into simulations (very low abundant ones)\n",
    "cutoff_value = float(0.0001)\n",
    "\n",
    "#Minimal coefficient,  See Micom publication for details.\n",
    "mincoef_value = float(1e-3)\n",
    "\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "model_folder = str('models_'+pro_ver)\n",
    "print(\"model_folder will be: \", model_folder)\n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Project name and version:\", pro_ver)\n",
    "print(\"---------------------------------------------------------\")\n",
    "os.chdir(working_dir)\n",
    "import os\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"filepath_crop_fmtas:\", filepath_crop_fmtas)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"data in filepath_crop_fmtas:\", data)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"filepath_condition:\", filepath_condition)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print('data_condition in filepath_condition:', data_condition)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"CPUS: \", cpus_used)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Model catalog/test_db\", test_db)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"medium filepath:\", test_medium)\n",
    "print(\"---------------------------------------------------------\")\n",
    "medium = load_qiime_medium(test_medium)\n",
    "print(\"medium: \", medium)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"filepath_allmetab, filepath to list of gapseq metabolites:\", filepath_allmetab)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"gapseq metabolites from filepath_allmetab: \", allmetab)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"cutoff_value: \", cutoff_value)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"mincoef_value: \", mincoef_value)\n",
    "print(\"---------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686970bf-fbfd-4a94-be17-548c060df19b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from micom import show_versions\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "def capture_output(function):\n",
    "    # Create a StringIO object to capture the output\n",
    "    output = io.StringIO()\n",
    "    # Use contextlib to redirect stdout to the StringIO object\n",
    "    with contextlib.redirect_stdout(output):\n",
    "        function()\n",
    "    # Get the captured output\n",
    "    return output.getvalue()\n",
    "\n",
    "# Capture the output of show_versions\n",
    "output_text = capture_output(show_versions)\n",
    "print(output_text)\n",
    "\n",
    "\n",
    "# Write the output to a text file\n",
    "with open(f\"{pro_ver}_versions.txt\", 'w') as f:\n",
    "    f.write(output_text)\n",
    "\n",
    "print(f\"{pro_ver}_versions.txt saved.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bd705-ad74-447f-a4ba-d1d1f8ef7638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure you can see all the columns separated nicely. If not, Jupyter may not be able to distinguish them properly!\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b6669-d524-4ed7-9bcf-5263256e26a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7fa6b-4f21-4498-a984-547a8a8abaad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cpus_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a90c0-eacf-45b0-be47-0eb83054182f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allmetab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b7dc2-3b13-40d6-8986-acf6e5a676f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66aaba7-d2ae-4de7-aa4b-7383e728bef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mincoef_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e03116-5f89-4f51-bc18-52c792db9a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1642659e-774c-4fae-8743-26b10befe4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collapsing abundances of equal taxa in the same sample to 1 row, adding all abundances\n",
    "# Make sure your abundance/taxonomy table has this structure! (if you go for genus level comparison)\n",
    "data_crop = data.groupby([\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"sample_id\"]).abundance.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbb6ea-ef04-42d4-9cea-6538bb64234b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2215633-e87e-4685-a6ab-c124d934e2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data_crop.head())\n",
    "data_crop.to_csv(f\"{pro_ver}_data_crop.csv\", sep=';', index=False)\n",
    "print(\"Saved CSV of data crop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a454805-c886-4fef-8639-94db76735d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68a9be-a484-4a5b-aae1-af7b3d825ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097f3ce-1114-46ae-ab2f-4b13e95373d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#data_condition, only those conditions you want to compare. You dont always want to compare ALL samples you have\n",
    "#                but instead only a few subgroups.\n",
    "#data_crop, all taxonomy files and abundances in a 1D structure. Contains all samples, but unique ids were collapsed to\n",
    "#           genus level and abundances were added.\n",
    "\n",
    "# Initialisieren des neuen DataFrame data_crop2 mit denselben Spalten wie data_crop, aber leer\n",
    "data_crop2 = pd.DataFrame(columns=data_crop.columns)\n",
    "\n",
    "# Durchlaufen aller Einträge in der Spalte 'sample' von data_condition\n",
    "for lookup_sample in data_condition['sample']:\n",
    "    # Suchen der Einträge in data_crop, die dem Wert in lookup_sample entsprechen\n",
    "    matches = data_crop[data_crop['sample_id'] == lookup_sample]\n",
    "    \n",
    "    # Hinzufügen der gefundenen Zeilen zum DataFrame data_crop2\n",
    "    data_crop2 = pd.concat([data_crop2, matches])\n",
    "\n",
    "    data_crop2.to_csv(f\"{pro_ver}_data_crop2.csv\", sep=';', index=False)\n",
    "print(\"Saved CSV of data crop2.\")\n",
    "# Anzeigen des resultierenden DataFrame\n",
    "data_crop2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc468889-2fe2-4c9e-9dcf-c1ab0311af40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946022f6-fff4-4fcd-99d3-c5971611e507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c05a3-c626-447b-b6aa-024d015a8250",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3/13 - Build model folder #############################################################################################\n",
    "#############################################################################################\n",
    "## May take longer!\n",
    "# Creates the sample.pickle for each sample in the folder defined above\n",
    "\n",
    "from micom.workflows import build\n",
    "\n",
    "manifest = build(data_crop2, out_folder=model_folder, model_db=test_db, cutoff=cutoff_value, threads=cpus_used, solver=\"cplex\")\n",
    "#manifest = build(data, out_folder=model_folder, model_db=test_db, cutoff=cutoff_value, threads=cpus_used)\n",
    "\n",
    "print(\"manifest:\", manifest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11c839-e479-4cbb-958c-a8eff059c725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a92ff-c22e-4b91-a16a-67665993e091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "from micom.workflows import tradeoff\n",
    "\n",
    "tradeoff_rates = tradeoff(manifest, model_folder=model_folder, medium=medium, threads=cpus_used)\n",
    "tradeoff_rates.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230ff0c-a171-42ca-a930-413499d04e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SAVING tradeoff data\n",
    "import pickle\n",
    "\n",
    "# CSV:\n",
    "print(tradeoff_rates.head())\n",
    "tradeoff_rates.to_csv(f\"{pro_ver}_tradeoff.csv\", sep=';', index=False)\n",
    "print(\"Saved CSV of tradeoff data\")\n",
    "\n",
    "# PICKLE:\n",
    "data_tuple = tuple(tradeoff_rates.itertuples(index=False, name=None))\n",
    "with open(f'{pro_ver}_tradeoff.pickle', 'wb') as f:\n",
    "    pickle.dump(data_tuple, f)\n",
    "print(\"Saved pickle of tradeoff data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f55c0a-c7da-438f-b7cc-b92d65b39ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QC, just to make sure the pickle file can be read again, in case you want to enter pipeline at this point\n",
    "\n",
    "with open(f'{pro_ver}_tradeoff.pickle', 'rb') as f:\n",
    "    loaded_tuple = pickle.load(f)\n",
    "\n",
    "# Umwandlung des Tupels in einen DataFrame\n",
    "# Usually stays like this. Reloading pickle of tradeoff manifest, defining the columns for the pickle\n",
    "columns = ['abundance', 'growth_rate', 'reactions', 'metabolites', 'taxon', 'tradeoff', 'sample_id']\n",
    "tradeoff_rates_df = pd.DataFrame(loaded_tuple, columns=columns)\n",
    "\n",
    "# Anzeigen der ersten Zeilen des DataFrames\n",
    "print(tradeoff_rates_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0507f-5e91-4935-b371-610398ffd654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "tradeoff_rates.groupby(\"tradeoff\").apply(\n",
    "    lambda df: (df.growth_rate > 1e-6).sum()).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7330734-d641-4d31-8b0f-de1c283a0471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from micom.viz import plot_tradeoff\n",
    "\n",
    "pl = plot_tradeoff(tradeoff_rates, filename=f\"{pro_ver}_tradeoff.html\")\n",
    "\n",
    "print(f\"Tradeoff of {pro_ver} visualized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194c350-c26e-45de-b60a-88aa14c322fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine tradeoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575458c0-cf15-4d25-8acc-ee901ff3eb93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tradeoff_value = float(1.0)\n",
    "print(\"tradeoff_value: \", tradeoff_value)\n",
    "print(\"---------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe402039-d967-4fce-8232-ae698d99ec06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#4/13 - grow function #############################################################################################\n",
    "#############################################################################################\n",
    "## May take longer!\n",
    "\n",
    "# Simulates the microbiome!\n",
    "\n",
    "from micom.workflows import grow\n",
    "\n",
    "res = grow(manifest, model_folder=model_folder, medium=medium, tradeoff=tradeoff_value, threads=cpus_used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acdd500-1b86-4497-bcd9-3f232a1c830c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Log-Info########\n",
    "import os\n",
    "print(os.getcwd())\n",
    "from datetime import datetime\n",
    "# aktuelles Datum und Uhrzeit\n",
    "now = datetime.now()\n",
    "print(\"Aktuelles Datum und Uhrzeit:\", now)\n",
    "### Log-Info End########\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Speichere das frische Tuple aus \"res\" in einer Pickle-Datei\n",
    "with open(f\"{pro_ver}_results.pickle\", \"wb\") as f:\n",
    "    pickle.dump(res, f)\n",
    "\n",
    "# Lade das Tuple aus der Pickle-Datei\n",
    "with open(f\"{pro_ver}_results.pickle\", \"rb\") as f:\n",
    "    loaded_pickle_tuple = pickle.load(f)\n",
    "\n",
    "# Überprüfe das geladene Tuple - es sollte genauso aussehen wie \"res\"\n",
    "print(loaded_pickle_tuple)\n",
    "\n",
    "#### ZUSÄTZLICH ALS REINE CSV-Information, um mal reinzuschauen\n",
    "#### Kann NICHT wieder eingeladen werden\n",
    "df = pd.DataFrame(res[0])\n",
    "df.to_csv(pro_ver+'_res0.csv', index=True, header=True, sep=';')\n",
    "df = pd.DataFrame(res[1])\n",
    "df.to_csv(pro_ver+'_res1.csv', index=True, header=True, sep=';')\n",
    "df = pd.DataFrame(res[2])\n",
    "df.to_csv(pro_ver+'_res2.csv', index=True, header=True, sep=';')\n",
    "\n",
    "### Log-Info########\n",
    "import os\n",
    "print(os.getcwd())\n",
    "from datetime import datetime\n",
    "# aktuelles Datum und Uhrzeit\n",
    "now = datetime.now()\n",
    "print(\"Aktuelles Datum und Uhrzeit:\", now)\n",
    "### Log-Info End########\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fffa46-774d-4e4a-a7a3-75580810b6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa5b21-e51c-4a6b-960b-4ad3833e7a93",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#10/13 - Load the two chosen conditions ########################################################\n",
    "# First column contains the \"sample id\", read it into one column dataframe\n",
    "data_condition_samp = data_condition[[data_condition.columns[0]]]\n",
    "print(data_condition_samp)\n",
    "\n",
    "# Second column contains the \"condition\", read it into one column dataframe\n",
    "data_condition_col = data_condition[[data_condition.columns[1]]]\n",
    "print(data_condition_col)\n",
    "\n",
    "#A little hack - store it as csv file in the base jupyter folder\n",
    "df = pd.DataFrame(data_condition_col)\n",
    "df.to_csv(f'{pro_ver}_temp_condition_only.csv', index=False, header=True)\n",
    "\n",
    "df = pd.DataFrame(data_condition_samp)\n",
    "df.to_csv(f'{pro_ver}_temp_sample_only.csv', index=False, header=True)\n",
    "\n",
    "# Read each CSV file as DataFrame\n",
    "meta_sample_path = f'{pro_ver}_temp_sample_only.csv'\n",
    "meta_sample = pd.read_csv(meta_sample_path, header=0)\n",
    "# Manually convert DataFrame to Series if it contains only one column\n",
    "if meta_sample.shape[1] == 1:\n",
    "    meta_sample = meta_sample.iloc[:, 0]\n",
    "print(\"meta_sample:\", meta_sample)\n",
    "\n",
    "meta_condition_path = f'{pro_ver}_temp_condition_only.csv'\n",
    "meta_condition = pd.read_csv(meta_condition_path, header=0)\n",
    "# Manually convert DataFrame to Series if it contains only one column\n",
    "if meta_condition.shape[1] == 1:\n",
    "    meta_condition = meta_condition.iloc[:, 0]\n",
    "print(\"meta_condition:\", meta_condition)\n",
    "\n",
    "# Merge the two Series by setting the index of the 'meta_condition' Series to the values of the 'meta_sample' Series\n",
    "meta_condition.index = meta_sample\n",
    "print(\"meta_condition NEW:\", meta_condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b5bb4-8a1d-4bc5-9c2d-17532941d038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from micom.measures import production_rates\n",
    "\n",
    "#production_rate:\n",
    "print(\"---> Performing production_rates() with:\", pro_ver)\n",
    "prod_rates = production_rates(res)\n",
    "ind_prod_rates_path = f\"{pro_ver}_prod_rates.csv\"\n",
    "\n",
    "\n",
    "df = pd.DataFrame(prod_rates)\n",
    "df.to_csv(ind_prod_rates_path, index=True, header=True)\n",
    "print(\"---- FINISHED production_rates()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c1cbf-edc6-473f-ba73-7f0d97629a72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modifying production_rate:\n",
    "modified_prod_rates = prod_rates.copy()\n",
    "\n",
    "# Erstellen einer neuen Spalte 'conditions' in modified_prod_rates\n",
    "modified_prod_rates['conditions'] = ''\n",
    "\n",
    "# Durchgehen jeder Zeile in modified_prod_rates\n",
    "for index, row in modified_prod_rates.iterrows():\n",
    "    # Finden der Übereinstimmung über die samples (TK_016)\n",
    "    match = data_condition[data_condition['sample'] == row['sample_id']]\n",
    "\n",
    "    # Wenn eine Übereinstimmung gefunden wurde, wird der Wert übertragen\n",
    "    if not match.empty:\n",
    "        modified_prod_rates.at[index, 'conditions'] = match['condition'].values[0]\n",
    "\n",
    "ind_modprod_path = f\"{pro_ver}_mod_prod_rate.csv\"\n",
    "# Speichern des modified_prod_rates DataFrame als CSV-Datei\n",
    "modified_prod_rates.to_csv(ind_modprod_path, index=False, sep=\";\")\n",
    "print(f'Created {ind_modprod_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903610b4-c3dd-4214-8248-223fd0d3d914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modified_prod_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b6d41e-187c-4c6e-a622-bd160a74f73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from micom.stats import compare_groups\n",
    "#compare_groups:\n",
    "\n",
    "ind_comp_group_path = f\"{pro_ver}_compare_groups.csv\"\n",
    "print(\"---> Performing compare_groups() with:\", ind_comp_group_path)\n",
    "\n",
    "tests = compare_groups(modified_prod_rates, \"conditions\", groups=[\"d13_control\", \"d13_CHX207\"])  \n",
    "# \"group\" ist der name der Spalte mit den Konditionen\n",
    "# groups=[\"healthy\", \"cachexia\"] 2\n",
    "# d01_chx erster hit\n",
    "# d13_chx zweiter hit\n",
    "# groups=[\"d01_chx\", \"d13_chx\"]\n",
    "# log2_fold_change positiv --> gruppe ZWEI = d13_chx CHX erhöht.\n",
    "# log2_fold_change negativ --> gruppe EINS = d01_chx gesund erhöht.\n",
    "\n",
    "tests.sort_values(by=\"q\")\n",
    "tests.to_csv(ind_comp_group_path, index=False, sep=\";\")\n",
    "print(f\"---- FINISHED compare_groups() with {ind_comp_group_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c1a2d-eac2-4971-9353-979033b3cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "print(f'filepath_allmetab = {filepath_allmetab}')\n",
    "      \n",
    "metab_df = pd.read_csv(filepath_allmetab, sep=\";\")\n",
    "\n",
    "# Erstelle ein Wörterbuch zur Abbildung der id_metab/id zu metabolite/name\n",
    "replacement_dict = pd.Series(metab_df.name.values, index=metab_df.id).to_dict()\n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc575e3-e7e3-4523-a892-48a65c43102d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c65dc-ff92-4bff-996f-ac77351aece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_id_filename = f\"{pro_ver}_compare_groups.csv\"\n",
    "print(\"----> Screening CSV:\", change_id_filename)\n",
    "# Lese die Datei als DataFrame\n",
    "df = pd.read_csv(os.path.join(working_dir, change_id_filename))\n",
    "# Ersetze die alten Metaboliten-IDs durch die neuen\n",
    "df.replace(replacement_dict, regex=True, inplace=True)\n",
    "\n",
    "# ----> INSERT: Saving modified CSV in a desired folder:\n",
    "df.to_csv(os.path.join(working_dir, 'mIDs_' + change_id_filename), index=False)\n",
    "print(\"---- mIDs-modified CSV written.\", 'mIDs_' + change_id_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e298d-9ef0-456b-a965-6d926a4c39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_id_filename = f\"{pro_ver}_res0.csv\"\n",
    "print(\"----> Screening CSV:\", change_id_filename)\n",
    "# Lese die Datei als DataFrame\n",
    "df = pd.read_csv(os.path.join(working_dir, change_id_filename))\n",
    "# Ersetze die alten Metaboliten-IDs durch die neuen\n",
    "df.replace(replacement_dict, regex=True, inplace=True)\n",
    "\n",
    "# ----> INSERT: Saving modified CSV in a desired folder:\n",
    "df.to_csv(os.path.join(working_dir, 'mIDs_' + change_id_filename), index=False)\n",
    "print(\"---- mIDs-modified CSV written.\", 'mIDs_' + change_id_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b8bc1-99bc-4fa9-b6ec-9ce7d8af5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_id_filename = f\"{pro_ver}_res1.csv\"\n",
    "print(\"----> Screening CSV:\", change_id_filename)\n",
    "# Lese die Datei als DataFrame\n",
    "df = pd.read_csv(os.path.join(working_dir, change_id_filename))\n",
    "# Ersetze die alten Metaboliten-IDs durch die neuen\n",
    "df.replace(replacement_dict, regex=True, inplace=True)\n",
    "\n",
    "# ----> INSERT: Saving modified CSV in a desired folder:\n",
    "df.to_csv(os.path.join(working_dir, 'mIDs_' + change_id_filename), index=False)\n",
    "print(\"---- mIDs-modified CSV written.\", 'mIDs_' + change_id_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa317c-2dfa-4f3a-98d2-1b9321f0248e",
   "metadata": {},
   "source": [
    "# Script Micom run2\n",
    "Finished!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qiime2-2024.2_ampl-Micom0.35.0",
   "language": "python",
   "name": "qiime2-2024.2_ampl-micom0.35.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
