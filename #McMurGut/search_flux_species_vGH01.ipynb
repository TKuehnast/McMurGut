{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8733c9b-1586-42fc-97ed-3c9d44a4e390",
   "metadata": {},
   "source": [
    "# Script Search flux species\n",
    "Version GitHub 01\n",
    "\n",
    "  \n",
    "Previously:  \n",
    "You simulated a community for groups of samples (sick vs healthy) in Micom.\n",
    "You used compare_groups() to see differential metabolite production for sick vs healthy.\n",
    "You did see a differential flux, but want to track it down to those taxa that produce these\n",
    "differentially produced metabolites.\n",
    "\n",
    "Prepare:\n",
    "- Res[1] from Micoms grow() as CSV\n",
    "- Gapseq/ModelSEED CSV file with metabolite IDs (column id) and corresponding metabolite names (column name)\n",
    "- Condition CSV file with sample names (column sample) and conditions A or B (column condition)\n",
    "- Make sure CSV files are loaded and stored with correct separator \";\" or \",\"\n",
    "- Later on, make sure conditionA and conditionB are dedicated to the correct conditions\n",
    "\n",
    "Procedure:\n",
    "- Extracts all metabolite names (cpd***) present in your community and stores it in a list\n",
    "- Screen for each *exported* metabolite, saving all genera that produced it, including flux, abundance, flux\\*abundance and sample ID in a unique CSV file\n",
    "- Add the sample's condition in an additional column, based on your condition file\n",
    "- Combine flux, abundances and flux\\*abundances from each taxa and condition (Alistipes in healthy samples has a total sum of flux of 66). \n",
    "- Translate Gapseq/Modelseed metabolite IDs to metabolic names\n",
    "- Merge condition A (like healthy) with condition B (like disease) for each metabolite, to contain both (mIDs_metabolite_fluxsummerge.csv)\n",
    "\n",
    "\n",
    "\n",
    "**Output files:**\n",
    "Aside of a lot of intermediate files, you will get 9 CSV files:\n",
    "- All flux*abundances from all taxa, in condition A and in condition B with separte columns (project_genusfluxmeta_abundflux.csv)\n",
    "- All flux*abundances from all taxa, with condition B subtracted from condition A (project_genusfluxmeta_abundflux_diff.csv)\n",
    "- All flux*abundances from all taxa, with column headers simplified (project_genusfluxmeta_abundflux_diffreduced.csv)\n",
    "- Same triplet for flux, and abundance.\n",
    "  \n",
    "**Value interpretation:**  \n",
    "The difference of SICK (sick_cond) MINUS CONTROL (control_cond) will be calculated. (checkpoints for this later on)  \n",
    "A +POSITIVE value in _diff.csv and _diffreduced.csv means higher flux/abundance/flux*abundance in DISEASE  \n",
    "A -NEGATIVE value in _diff.csv and _diffreduced.csv means a reduced flux in DISEASE  \n",
    "  \n",
    "########################################################  \n",
    "By Torben Kuehnast, torben.kuehnast@gmail.com, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5483ea88-5339-4715-8daf-cd175e4d49b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a453f-2721-4c4b-ab2e-27520f1bf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT THE CORRECT FILE NAMES HERE ####\n",
    "### INSERT THE CORRECT FILE NAMES HERE ####\n",
    "### INSERT THE CORRECT FILE NAMES HERE ####\n",
    "### INSERT THE CORRECT FILE NAMES HERE ####\n",
    "\n",
    "\n",
    "\n",
    "# INSERT: Where files are created\n",
    "working_dir = \"/home/working_directory\"\n",
    "\n",
    "# INSERT: the folder where the result of build and grow are, the res[1] file!\n",
    "# ... its basic path / working directory\n",
    "source_path = \"/home/micoms_res1_directory\"\n",
    "\n",
    "# Split res_file into the <project name part> and <_res1.csv> ... (needed to be able to apply proper naming of newly created files)\n",
    "condition_folder_part = \"projectnamepart_withoutending\"\n",
    "\n",
    "# ... Automatically, this should lead to the name of the RES file. It should precisely look like this.\n",
    "target_res1 = f\"{condition_folder_part}_res1.csv\"\n",
    "\n",
    "# INSERT: Choose a name for the automatically created file where only the list of metabolites found in res1 are listed:\n",
    "only_metab_list = \"all_metabolites_project.csv\"\n",
    "\n",
    "\n",
    "# INSERT: Path and filename of condition file comparing A to B with sample names and condition names?\n",
    "#Looking like:\n",
    "#sample,condition\n",
    "#sample45,healthy\n",
    "#sample46,healthy\n",
    "#sample47,sick\n",
    "condition_file = \"/home/condition_directory/projectname_conditionfileAvsB.csv\"\n",
    "\n",
    "\n",
    "# INSERT: Define column headers for the new CSVs. Should be like this. \n",
    "headers = ['taxon', 'sample_id', 'tolerance', 'reaction', 'flux', 'abundance', 'metabolite', 'direction', 'origin']\n",
    "\n",
    "\n",
    "# INSERT: File where ALL gapseq metabolites are listed with cpd ID and real metabolite name\n",
    "filepath_allmetab = '/home/all_gapseq_metabolites_756_compgr.csv'\n",
    "\n",
    "\n",
    "# INSERT: Final genus_flux_metabolite output file name, possibly stay like that.\n",
    "genus_flux_metabolites_var = f\"{condition_folder_part}_genusfluxmeta\"\n",
    "\n",
    "\n",
    "# INSERT: Make sure that all \";\" and \",\" separation of your CSV files fit to the commands below\n",
    "\n",
    "### insert END ###\n",
    "### insert END ###\n",
    "### insert END ###\n",
    "### insert END ###\n",
    "### insert END ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356246b-48a3-47d1-9d4d-466871b1e421",
   "metadata": {},
   "source": [
    "# Part 1 - finding all metabolites involved \n",
    "\n",
    "This script is based on metabolic ID nomenclature from modelSEED  \n",
    "Found in gapseq, ssniff diet file and Micom output.  \n",
    "https://modelseed.org/biochem/compounds   \n",
    "https://github.com/jotech/gapseq  \n",
    "Script collects all cpd**** metabolite IDs that have been created throughout the simulation in  \n",
    "one metabolite file, a simple list:  \n",
    "\n",
    "    metabolite  \n",
    "    cpd00001  \n",
    "    cpd00002  \n",
    "    cpd00009  \n",
    "    cpd00013  \n",
    "    ...  \n",
    "\n",
    "INPUT: Products of Micom's grow() function, in res[1]!, stored as CSV  \n",
    "  \n",
    "OUTOUT: all_metabolites.csv  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1ff42-8418-4927-b5f0-82445c6ac5fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# processing, Extracting the conditions from condition folder automatically.\n",
    "df = pd.read_csv(condition_file, delimiter=';')\n",
    "# processing: Extract the unique condition names\n",
    "unique_conditions = df['condition'].unique()\n",
    "print(\"unique_conditions:\", unique_conditions)\n",
    "# processing: Ensure there are only two unique conditions\n",
    "if len(unique_conditions) != 2:\n",
    "    raise ValueError(\"There are more than two unique condition names!\")\n",
    "# processing: Store the unique condition names in variables\n",
    "conditionpartA, conditionpartB = unique_conditions\n",
    "# processing: Create DataFrames based on the condition parts\n",
    "condition_filter_A = df[df['condition'] == conditionpartA]\n",
    "condition_filter_B = df[df['condition'] == conditionpartB]\n",
    "# processing: Display the results\n",
    "print(\"Condition Filter A:\")\n",
    "display(condition_filter_A)\n",
    "print(\"Condition Filter B:\")\n",
    "display(condition_filter_B)\n",
    "\n",
    "\n",
    "# processing\n",
    "csv_path = os.path.join(working_dir, only_metab_list)\n",
    "if not os.path.exists(csv_path):\n",
    "    pd.DataFrame(columns=['metabolite']).to_csv(csv_path, sep=';', index=False)\n",
    "\n",
    "#processing\n",
    "condition_folder_core = condition_folder_part\n",
    "\n",
    "#conditionABfolder\n",
    "conditionAfolder = f\"{condition_folder_part}_{conditionpartA}\"\n",
    "conditionBfolder = f\"{condition_folder_part}_{conditionpartB}\"\n",
    "\n",
    "# The folder where condion A and B are placed\n",
    "condition_folder_partA = conditionAfolder\n",
    "condition_folder_partB = conditionBfolder\n",
    "\n",
    "# The folder where condion A and B are placed RENAMED with mIDs\n",
    "condition_folder_partAm = f\"{conditionAfolder}_mIDs\"\n",
    "condition_folder_partBm = f\"{conditionBfolder}_mIDs\"\n",
    "merged_conditions = f\"{condition_folder_part}_merged\"\n",
    "    \n",
    "# Read existing metabolites to avoid duplicates\n",
    "metabolites_df = pd.read_csv(csv_path, sep=';')\n",
    "\n",
    "# Set to store all unique metabolites\n",
    "unique_metabolites = set(metabolites_df['metabolite'])\n",
    "\n",
    "# Function to clean metabolite names\n",
    "def clean_metabolite_name(name):\n",
    "    return re.sub(r'(_e0|_m)$', '', name)\n",
    "\n",
    "# Search and update CSV\n",
    "for root, dirs, files in os.walk(source_path):\n",
    "    if condition_folder_part in root:\n",
    "        print(\"condition_folder_part found:\", condition_folder_part)\n",
    "        print(\"root is:\", root)\n",
    "        for file in files:\n",
    "            if file == target_res1:\n",
    "                print(\"---> found it!\", file, \" looking for \", target_res1)\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path, sep=';')\n",
    "                metabolites_in_file = df['metabolite'].unique()\n",
    "                for metabolite in metabolites_in_file:\n",
    "                    clean_metabolite = clean_metabolite_name(metabolite)\n",
    "                    if clean_metabolite not in unique_metabolites:\n",
    "                        unique_metabolites.add(clean_metabolite)\n",
    "                        print(\"Added the following metabolite: \", clean_metabolite)\n",
    "\n",
    "# Convert set to DataFrame\n",
    "new_metabolites_df = pd.DataFrame(list(unique_metabolites), columns=['metabolite'])\n",
    "\n",
    "# Sort and save metabolite list\n",
    "new_metabolites_df.sort_values(by='metabolite', inplace=True)\n",
    "new_metabolites_df.to_csv(csv_path, sep=';', index=False)\n",
    "\n",
    "\n",
    "# Save separated condition A and B\n",
    "condApath = os.path.join(working_dir, f\"{conditionpartA}.csv\")\n",
    "condition_filter_A.to_csv(condApath, sep=';', index=False)\n",
    "condBpath = os.path.join(working_dir, f\"{conditionpartB}.csv\")\n",
    "condition_filter_A.to_csv(condBpath, sep=';', index=False)\n",
    "\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb0463-491d-4fc8-923f-e0b223683683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conditionpartA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3483af-2556-4ab7-9aea-0e6360ac51a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conditionpartB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d099eb-a108-4555-952f-aa72ac1a71ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: AFTER running the condition A/B extraction, look what is A and B.\n",
    "# Look at conditionpartA and its samples, and define in the following that it is either sick_cond or control_cond\n",
    "# \n",
    "\n",
    "# This is important when later on substracting flux*abundance of methane's control (like value = 1) from methane in cachexia (like value = 3), \n",
    "# leading to (3-1=) +2 flux*abundance in cacheixa\n",
    "\n",
    "# SICK condition:\n",
    "\n",
    "#sick_cond = conditionpartA\n",
    "sick_cond = conditionpartB\n",
    "\n",
    "\n",
    "\n",
    "# HEALTHY condition:\n",
    "\n",
    "control_cond = conditionpartA\n",
    "#control_cond = conditionpartB\n",
    "\n",
    "\n",
    "print(\"sick condition is:\", sick_cond)\n",
    "print(\"control coondition is:\", control_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee62c0-9750-4d1a-b5b2-9563e18db4e1",
   "metadata": {},
   "source": [
    "# PART 2A - collecting fluxes to the respective metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee551d7-3df9-4d24-a5b0-9da953102307",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### For conditionA <-- (all healthy or disease samples)\n",
    "\n",
    "# Define full path to the subfolder\n",
    "full_subfolder_path = os.path.join(working_dir, conditionAfolder)\n",
    "\n",
    "# Ensure the subfolder exists\n",
    "if not os.path.exists(full_subfolder_path):\n",
    "    os.makedirs(full_subfolder_path)\n",
    "\n",
    "# Load the list of metabolites\n",
    "metabolites_df = pd.read_csv(csv_path, sep=';')\n",
    "\n",
    "# Iterate through each metabolite\n",
    "for metabolite in metabolites_df['metabolite']:\n",
    "    print(\"Processing:\", metabolite)\n",
    "    all_rows = []  # Initialize a list to collect rows\n",
    "    \n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        if condition_folder_part in root:\n",
    "            for file in files:\n",
    "                if file == target_res1:\n",
    "                    with open(os.path.join(root, file), mode='r', encoding='utf-8') as f:\n",
    "                        reader = csv.DictReader(f, delimiter=';')\n",
    "                        for row in reader:\n",
    "                            # Check if the row's sample_id matches any sample in the condition_filter_A\n",
    "                            if row['sample_id'] in condition_filter_A['sample'].values:\n",
    "                                if metabolite in row['metabolite']:\n",
    "                                    row['origin'] = os.path.basename(root)\n",
    "                                    all_rows.append(row)  # Append the row to the list if it matches the filter\n",
    "    \n",
    "    # Create a DataFrame from the collected rows if there are any\n",
    "    if all_rows:\n",
    "        metabolite_df = pd.DataFrame(all_rows, columns=headers)\n",
    "        # Save the DataFrame for the current metabolite to a CSV file\n",
    "        metabolite_df.to_csv(os.path.join(full_subfolder_path, f'{metabolite}_{conditionpartA}.csv'), sep=';', index=False)\n",
    "        print(f'{metabolite}_{conditionpartA}.csv created in {full_subfolder_path}')\n",
    "\n",
    "        \n",
    "print(\"Finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7eaca-fcb0-47ba-af62-b7541a0382ff",
   "metadata": {},
   "source": [
    "# PART 2B - collecting fluxes to the respective metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784674b-a1db-4ef2-b57c-1cc550666a66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### For conditionB <-- (all disease or healthy samples)\n",
    "\n",
    "# Define full path to the subfolder\n",
    "full_subfolder_path = os.path.join(working_dir, conditionBfolder)\n",
    "\n",
    "# Ensure the subfolder exists\n",
    "if not os.path.exists(full_subfolder_path):\n",
    "    os.makedirs(full_subfolder_path)\n",
    "\n",
    "# Load the list of metabolites\n",
    "metabolites_df = pd.read_csv(csv_path, sep=';')\n",
    "\n",
    "# Iterate through each metabolite\n",
    "for metabolite in metabolites_df['metabolite']:\n",
    "    print(\"Processing:\", metabolite)\n",
    "    all_rows = []  # Initialize a list to collect rows\n",
    "    \n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        if condition_folder_part in root:\n",
    "            for file in files:\n",
    "                if file == target_res1:\n",
    "                    with open(os.path.join(root, file), mode='r', encoding='utf-8') as f:\n",
    "                        reader = csv.DictReader(f, delimiter=';')\n",
    "                        for row in reader:\n",
    "                            # Check if the row's sample_id matches any sample in the condition_filter_B\n",
    "                            if row['sample_id'] in condition_filter_B['sample'].values:\n",
    "                                if metabolite in row['metabolite']:\n",
    "                                    row['origin'] = os.path.basename(root)\n",
    "                                    all_rows.append(row)  # Append the row to the list if it matches the filter\n",
    "    \n",
    "    # Create a DataFrame from the collected rows if there are any\n",
    "    if all_rows:\n",
    "        metabolite_df = pd.DataFrame(all_rows, columns=headers)\n",
    "        # Save the DataFrame for the current metabolite to a CSV file\n",
    "        metabolite_df.to_csv(os.path.join(full_subfolder_path, f'{metabolite}_{conditionpartB}.csv'), sep=';', index=False)\n",
    "        print(f'{metabolite}_{conditionpartB}.csv created in {full_subfolder_path}')\n",
    "\n",
    "        \n",
    "print(\"Finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f391925-98dc-4be5-8266-8892823844c1",
   "metadata": {},
   "source": [
    "#  Part 3 - polish flux file, add conditions to each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c78b0-0396-4d3f-a054-37be3f65096e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove medium rows\n",
    "# Remove import rows\n",
    "# Add condition based on a CSV file showing sample and conditions\n",
    "# Calculate flux*abundance\n",
    "# For conditionA, then conditionB\n",
    "\n",
    "\n",
    "\n",
    "# Condition-Datei laden\n",
    "condition_df = pd.read_csv(condition_file, sep=\";\")\n",
    "\n",
    "# Arbeitsverzeichnis für die Screening-Dateien festlegen\n",
    "screening_folderA = os.path.join(working_dir, condition_folder_partA)\n",
    "\n",
    "# Alle CSV-Dateien im Screening-Ordner durchlaufen\n",
    "for filename in os.listdir(screening_folderA):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(screening_folderA, filename)\n",
    "        \n",
    "        # Screening-Datei laden\n",
    "        df = pd.read_csv(file_path, sep=\";\")\n",
    "        \n",
    "        # Rows löschen, die \"medium\" in der \"taxon\"-Spalte haben\n",
    "        df = df[df['taxon'] != 'medium']\n",
    "        \n",
    "        # Rows löschen, die \"import\" in der \"direction\"-Spalte haben\n",
    "        df = df[df['direction'] != 'import']\n",
    "        \n",
    "        # Neue Spalte \"condition\" erstellen und Werte aus condition_file übernehmen\n",
    "        df['condition'] = df['sample_id'].map(condition_df.set_index('sample')['condition'])\n",
    "        \n",
    "        # Neue Spalte \"flux_abundance\" erstellen und Werte berechnen\n",
    "        df['flux_abundance'] = df['flux'] * df['abundance']\n",
    "        \n",
    "        # Datei speichern\n",
    "        df.to_csv(file_path, sep=\";\", index=False)\n",
    "\n",
    "print(f\"Die Verarbeitung der Dateien von condition {conditionpartA} ist abgeschlossen.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Arbeitsverzeichnis für die Screening-Dateien festlegen\n",
    "screening_folderB = os.path.join(working_dir, condition_folder_partB)\n",
    "\n",
    "# Alle CSV-Dateien im Screening-Ordner durchlaufen\n",
    "for filename in os.listdir(screening_folderB):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(screening_folderB, filename)\n",
    "        \n",
    "        # Screening-Datei laden\n",
    "        df = pd.read_csv(file_path, sep=\";\")\n",
    "        \n",
    "        # Rows löschen, die \"medium\" in der \"taxon\"-Spalte haben\n",
    "        df = df[df['taxon'] != 'medium']\n",
    "        \n",
    "        # Rows löschen, die \"import\" in der \"direction\"-Spalte haben\n",
    "        df = df[df['direction'] != 'import']\n",
    "        \n",
    "        # Neue Spalte \"condition\" erstellen und Werte aus condition_file übernehmen\n",
    "        df['condition'] = df['sample_id'].map(condition_df.set_index('sample')['condition'])\n",
    "        \n",
    "        # Neue Spalte \"flux_abundance\" erstellen und Werte berechnen\n",
    "        df['flux_abundance'] = df['flux'] * df['abundance']\n",
    "        \n",
    "        # Datei speichern\n",
    "        df.to_csv(file_path, sep=\";\", index=False)\n",
    "\n",
    "print(f\"Die Verarbeitung der Dateien von condition {conditionpartB} ist abgeschlossen.\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9b2f1-93fe-4e4d-8f17-5532f67c5558",
   "metadata": {},
   "source": [
    "# Part 4 - calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ba548-8c84-445f-9136-70bb8f887b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Part 4 \n",
    "\n",
    "# Add sums of flux, abundance and flux_abundance\n",
    "\n",
    "# Calculate a factor to normalize uneven condition sample sizes (like 4 healthy and 5 sick)\n",
    "# Multiply sums with factor, to get normalized values.\n",
    "\n",
    "# Re-sort, to be better and more quickly added to statistics program\n",
    "\n",
    "\n",
    "# Condition-Datei laden\n",
    "condition_df = pd.read_csv(condition_file, sep=\";\")\n",
    "\n",
    "# Zähle die Einträge in der Spalte \"condition\"\n",
    "conditionpartA_count = condition_df['condition'].value_counts().get(conditionpartA, 0)\n",
    "conditionpartB_count = condition_df['condition'].value_counts().get(conditionpartB, 0)\n",
    "\n",
    "# Finde den niedrigeren Wert und berechne die Faktoren\n",
    "condition_min_count = min(conditionpartA_count, conditionpartB_count)\n",
    "condA_factor = condition_min_count / conditionpartA_count if conditionpartA_count else 0\n",
    "condB_factor = condition_min_count / conditionpartB_count if conditionpartB_count else 0\n",
    "\n",
    "def process_files(screening_folder, factor, condition_suffix):\n",
    "    for filename in os.listdir(screening_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(screening_folder, filename)\n",
    "            \n",
    "            # Screening-Datei laden\n",
    "            df = pd.read_csv(file_path, sep=\";\")\n",
    "            \n",
    "            # Gruppieren und zusammenfassen\n",
    "            grouped = df.groupby(['taxon', 'reaction', 'metabolite', 'origin', 'condition', 'direction']).agg({\n",
    "                'flux': 'sum',\n",
    "                'abundance': 'sum',\n",
    "                'flux_abundance': 'sum'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Faktoren anwenden und umbenennen\n",
    "            grouped[f'flux_sum_norm_{condition_suffix}'] = grouped['flux'] * factor\n",
    "            grouped[f'abundance_sum_norm_{condition_suffix}'] = grouped['abundance'] * factor\n",
    "            grouped[f'flux_abundance_sum_norm_{condition_suffix}'] = grouped['flux_abundance'] * factor\n",
    "            \n",
    "            # Umbenennen der ursprünglichen Summenspalten\n",
    "            grouped.rename(columns={\n",
    "                'flux': 'flux_sum',\n",
    "                'abundance': 'abundance_sum',\n",
    "                'flux_abundance': 'flux_abundance_sum'\n",
    "            }, inplace=True)\n",
    "            \n",
    "            # Gesamtwerte hinzufügen\n",
    "            total_flux_sum = grouped['flux_sum'].sum()\n",
    "            total_abundance_sum = grouped['abundance_sum'].sum()\n",
    "            total_flux_abundance_sum = grouped['flux_abundance_sum'].sum()\n",
    "            \n",
    "            total_flux_sum_norm = grouped[f'flux_sum_norm_{condition_suffix}'].sum()\n",
    "            total_abundance_sum_norm = grouped[f'abundance_sum_norm_{condition_suffix}'].sum()\n",
    "            total_flux_abundance_sum_norm = grouped[f'flux_abundance_sum_norm_{condition_suffix}'].sum()\n",
    "            \n",
    "            total_row = pd.DataFrame({\n",
    "                'taxon': ['Total'],\n",
    "                'flux_sum': [total_flux_sum],\n",
    "                'abundance_sum': [total_abundance_sum],\n",
    "                'flux_abundance_sum': [total_flux_abundance_sum],\n",
    "                f'flux_sum_norm_{condition_suffix}': [total_flux_sum_norm],\n",
    "                f'abundance_sum_norm_{condition_suffix}': [total_abundance_sum_norm],\n",
    "                f'flux_abundance_sum_norm_{condition_suffix}': [total_flux_abundance_sum_norm],\n",
    "                'reaction': [''],\n",
    "                'metabolite': [''],\n",
    "                'origin': [''],\n",
    "                'condition': [''],\n",
    "                'direction': ['']\n",
    "            })\n",
    "            \n",
    "            result = pd.concat([grouped, total_row], ignore_index=True)\n",
    "            \n",
    "            # Spaltenreihenfolge anpassen\n",
    "            result = result[['taxon', \n",
    "                             f'flux_sum_norm_{condition_suffix}', \n",
    "                             f'abundance_sum_norm_{condition_suffix}', \n",
    "                             f'flux_abundance_sum_norm_{condition_suffix}', \n",
    "                             'reaction', \n",
    "                             'metabolite', \n",
    "                             'origin', \n",
    "                             'condition', \n",
    "                             'direction', \n",
    "                             'flux_sum', \n",
    "                             'abundance_sum', \n",
    "                             'flux_abundance_sum']]\n",
    "            \n",
    "            # Erstellen der neuen CSV mit \"_fluxsum.csv\"\n",
    "            new_filename = filename.replace(\".csv\", \"_fluxsum.csv\")\n",
    "            new_file_path = os.path.join(screening_folder, new_filename)\n",
    "            result.to_csv(new_file_path, sep=\";\", index=False)\n",
    "\n",
    "# Dateien in beiden Ordnern verarbeiten\n",
    "process_files(os.path.join(working_dir, condition_folder_partA), condA_factor, conditionpartA)\n",
    "print(f\"Die Verarbeitung der Dateien von condition {conditionpartA} ist abgeschlossen.\")\n",
    "\n",
    "process_files(os.path.join(working_dir, condition_folder_partB), condB_factor, conditionpartB)\n",
    "print(f\"Die Verarbeitung der Dateien von condition {conditionpartB} ist abgeschlossen.\")\n",
    "print(\"Finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c688e3-c59c-4267-8575-d4e5e717c68c",
   "metadata": {},
   "source": [
    "# Part 5A - replacing of IDs of \"cpd...\" to actual names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ed809-df09-4e3f-84b7-cd835321f835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Condition A\n",
    "\n",
    "# CHANGE THE IDS from cpd to REAL IDs. The change also modifies the filenames.\n",
    "\n",
    "# INPUT: merged_folder/<metabolite>_bothdif.csv _finalsum.csv _mergedsum.csv\n",
    "\n",
    "# OUTPUT: mIDs_merged/<REAL_metabolic_name>_*.csv\n",
    "\n",
    "def replace_ids_in_filename(filename, replacement_dict):\n",
    "    # For each metabolite ID in the dictionary, check if it's in the filename\n",
    "    for id_metab, metabolite in replacement_dict.items():\n",
    "        # If the ID is found in the filename, replace it with the metabolite name\n",
    "        if id_metab in filename:\n",
    "            filename = filename.replace(id_metab, metabolite)\n",
    "    return filename\n",
    "\n",
    "\n",
    "main_folder_path = os.path.join(working_dir, conditionAfolder)\n",
    "\n",
    "target_folder = condition_folder_partAm  # Change this to your specific subfolder name\n",
    "\n",
    "target_folder_path = os.path.join(working_dir, target_folder)\n",
    "\n",
    "# Ensure target directory exists\n",
    "if not os.path.exists(target_folder_path):\n",
    "    os.makedirs(target_folder_path)\n",
    "    print(f\"Created directory {target_folder_path}\")\n",
    "    \n",
    "\n",
    "# Read the mapping from 'id_metab' to 'metabolite' into a DataFrame\n",
    "metab_df = pd.read_csv(filepath_allmetab, sep=\";\")\n",
    "# Create a dictionary for replacements\n",
    "replacement_dict = pd.Series(metab_df.name.values, index=metab_df.id).to_dict()\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(main_folder_path):\n",
    "    for filename in filenames:\n",
    "        #if \"_bothdif.csv\" in filename:  # Filter to consider only specific files\n",
    "        if f\"_{conditionpartA}.csv\" in filename or f\"_{conditionpartA}_fluxsum.csv\" in filename:\n",
    "            # Generate a new filename by replacing metabolite IDs with names\n",
    "            new_filename = replace_ids_in_filename(filename, replacement_dict)\n",
    "            \n",
    "            # Construct the full filepath and the modified filepath\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            mod_filepath = os.path.join(target_folder_path, 'mIDs_' + new_filename)\n",
    "\n",
    "            if filename.endswith('.html'):\n",
    "                with open(filepath, 'r') as f:\n",
    "                    contents = f.read()\n",
    "\n",
    "                soup = BeautifulSoup(contents, 'lxml')\n",
    "                for item in metab_df.itertuples():\n",
    "                    for tag in soup.find_all(text=re.compile(re.escape(item.id_metab))):\n",
    "                        updated_string = tag.replace(item.id_metab, item.metabolite)\n",
    "                        tag.replace_with(updated_string)\n",
    "\n",
    "                # Save the modified HTML content to a new file\n",
    "                with open(mod_filepath, 'w') as f:\n",
    "                    f.write(str(soup))\n",
    "\n",
    "            elif filename.endswith('.csv'):\n",
    "                df = pd.read_csv(filepath)\n",
    "                df.replace(replacement_dict, regex=True, inplace=True)\n",
    "                # Save the modified DataFrame to a new CSV file\n",
    "                df.to_csv(mod_filepath, index=False)\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "print(\"Finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8b009-0aa0-4bd0-a0c4-465d2e4d9808",
   "metadata": {},
   "source": [
    "# Part 5B - replacing of IDs of \"cpd...\" to actual names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b4bd6-d93e-4a08-97de-2768f96ffd6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Condition B\n",
    "\n",
    "# CHANGE THE IDS from cpd to REAL IDs. The change also modifies the filenames.\n",
    "\n",
    "# INPUT: merged_folder/<metabolite>_bothdif.csv _finalsum.csv _mergedsum.csv\n",
    "\n",
    "# OUTPUT: mIDs_merged/<REAL_metabolic_name>_*.csv\n",
    "\n",
    "def replace_ids_in_filename(filename, replacement_dict):\n",
    "    # For each metabolite ID in the dictionary, check if it's in the filename\n",
    "    for id_metab, metabolite in replacement_dict.items():\n",
    "        # If the ID is found in the filename, replace it with the metabolite name\n",
    "        if id_metab in filename:\n",
    "            filename = filename.replace(id_metab, metabolite)\n",
    "    return filename\n",
    "\n",
    "\n",
    "main_folder_path = os.path.join(working_dir, conditionBfolder)\n",
    "\n",
    "target_folder = condition_folder_partBm  # Change this to your specific subfolder name\n",
    "\n",
    "target_folder_path = os.path.join(working_dir, target_folder)\n",
    "\n",
    "# Ensure target directory exists\n",
    "if not os.path.exists(target_folder_path):\n",
    "    os.makedirs(target_folder_path)\n",
    "    print(f\"Created directory {target_folder_path}\")\n",
    "    \n",
    "# Read the mapping from 'id_metab' to 'metabolite' into a DataFrame\n",
    "metab_df = pd.read_csv(filepath_allmetab, sep=\";\")\n",
    "# Create a dictionary for replacements\n",
    "replacement_dict = pd.Series(metab_df.name.values, index=metab_df.id).to_dict()\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(main_folder_path):\n",
    "    for filename in filenames:\n",
    "        #if \"_bothdif.csv\" in filename:  # Filter to consider only specific files\n",
    "        if f\"_{conditionpartB}.csv\" in filename or f\"_{conditionpartB}_fluxsum.csv\" in filename:\n",
    "            # Generate a new filename by replacing metabolite IDs with names\n",
    "            new_filename = replace_ids_in_filename(filename, replacement_dict)\n",
    "            \n",
    "            # Construct the full filepath and the modified filepath\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            mod_filepath = os.path.join(target_folder_path, 'mIDs_' + new_filename)\n",
    "\n",
    "            if filename.endswith('.html'):\n",
    "                with open(filepath, 'r') as f:\n",
    "                    contents = f.read()\n",
    "\n",
    "                soup = BeautifulSoup(contents, 'lxml')\n",
    "                for item in metab_df.itertuples():\n",
    "                    for tag in soup.find_all(text=re.compile(re.escape(item.id_metab))):\n",
    "                        updated_string = tag.replace(item.id_metab, item.metabolite)\n",
    "                        tag.replace_with(updated_string)\n",
    "\n",
    "                # Save the modified HTML content to a new file\n",
    "                with open(mod_filepath, 'w') as f:\n",
    "                    f.write(str(soup))\n",
    "\n",
    "            elif filename.endswith('.csv'):\n",
    "                df = pd.read_csv(filepath)\n",
    "                df.replace(replacement_dict, regex=True, inplace=True)\n",
    "                # Save the modified DataFrame to a new CSV file\n",
    "                df.to_csv(mod_filepath, index=False)\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "print(\"Finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13492f06-8cd1-4fd3-8773-0728a8e8cd90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(condition_folder_core)\n",
    "print(conditionpartA)\n",
    "print(conditionpartB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e7f9d-7a04-4030-b338-059ff2ce4e40",
   "metadata": {},
   "source": [
    "# Part 6 - merging conditions for each metabolite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8e54a-6ed1-40e3-8517-adaf7d0b2ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMBINE CHX and CONTROL metabolite files from the folders ending on _mIDs\n",
    "# You will get a merged folder with files ending on _fluxsummerge.csv\n",
    "\n",
    "# OUTPUT example: mIDs_Formate_MNXM39_fluxsummerge.csv\n",
    "#taxon\tflux_abundance_sum_norm_d13_CHX207\tflux_abundance_sum_norm_d13_control\tflux_sum_norm_d13_CHX207\tflux_sum_norm_d13_control\tabundance_sum_norm_d13_CHX207\tabundance_sum_norm_d13_control\n",
    "#Total\t0.357715421\t0.050616287\t34.69583001\t43.20324925\t1.474192454\t1.79060995\n",
    "#Stenotrophomonas\t0.008991778\t0.002472753\t32.15788288\t8.871002326\t0.000919222\t0.000222997\n",
    "#Parvibacter\t0\t0.00299169\t0\t2.852755745\t0\t0.000838961\n",
    "#Oscillibacter\t0\t0.001131594\t0\t0.139606914\t0\t0.022365999\n",
    "#...\n",
    "\n",
    "# Be aware that the values were normalized to the comparison variant with the fewer samples in the last step!\n",
    "\n",
    "# Erstellen des neuen Ordners für die zusammengeführten Dateien\n",
    "os.makedirs(os.path.join(working_dir, merged_conditions), exist_ok=True)\n",
    "\n",
    "def process_and_merge_files(partA_folder, partB_folder, output_folder, conditionA, conditionB):\n",
    "    # Durchsuchen des Ordners für conditionpartA\n",
    "    for filename in os.listdir(partA_folder):\n",
    "        if filename.startswith(\"mIDs_\") and filename.endswith(f\"{conditionA}_fluxsum.csv\"):\n",
    "            # Metabolite ID extrahieren\n",
    "            metabolite_id = filename[len(\"mIDs_\"):-len(f\"_{conditionA}_fluxsum.csv\")]\n",
    "\n",
    "            # Korrespondierende Datei in partB suchen\n",
    "            corresponding_file_B = f\"mIDs_{metabolite_id}_{conditionB}_fluxsum.csv\"\n",
    "            partB_filepath = os.path.join(partB_folder, corresponding_file_B)\n",
    "            \n",
    "            if os.path.exists(partB_filepath):\n",
    "                # Dateien einlesen\n",
    "                df_A = pd.read_csv(os.path.join(partA_folder, filename), sep=\";\")\n",
    "                df_B = pd.read_csv(partB_filepath, sep=\";\")\n",
    "\n",
    "                # DataFrames anpassen\n",
    "                df_A = df_A[['taxon', f'flux_sum_norm_{conditionA}', f'abundance_sum_norm_{conditionA}', f'flux_abundance_sum_norm_{conditionA}']]\n",
    "                df_B = df_B[['taxon', f'flux_sum_norm_{conditionB}', f'abundance_sum_norm_{conditionB}', f'flux_abundance_sum_norm_{conditionB}']]\n",
    "                \n",
    "                # Zusammenführen der DataFrames\n",
    "                merged_df = pd.merge(df_A, df_B, on='taxon', how='outer').fillna(0)\n",
    "\n",
    "                # Spaltenreihenfolge anpassen\n",
    "                merged_df = merged_df[['taxon', \n",
    "                                       f'flux_abundance_sum_norm_{conditionA}', f'flux_abundance_sum_norm_{conditionB}',\n",
    "                                       f'flux_sum_norm_{conditionA}', f'flux_sum_norm_{conditionB}',\n",
    "                                       f'abundance_sum_norm_{conditionA}', f'abundance_sum_norm_{conditionB}']]\n",
    "                \n",
    "                # Sortieren nach taxon alphabetisch von unten nach oben\n",
    "                merged_df = merged_df.sort_values(by='taxon', ascending=False).reset_index(drop=True)\n",
    "\n",
    "                # Zeile mit taxon \"Total\" an die zweite Position verschieben\n",
    "                total_row = merged_df[merged_df['taxon'] == 'Total']\n",
    "                other_rows = merged_df[merged_df['taxon'] != 'Total']\n",
    "                merged_df = pd.concat([total_row, other_rows]).reset_index(drop=True)\n",
    "\n",
    "                # Speichern der zusammengeführten und sortierten Datei\n",
    "                output_filename = f\"mIDs_{metabolite_id}_fluxsummerge.csv\"\n",
    "                merged_df.to_csv(os.path.join(output_folder, output_filename), sep=\";\", index=False)\n",
    "\n",
    "# Aufrufen der Funktion zum Verarbeiten und Zusammenführen der Dateien\n",
    "process_and_merge_files(\n",
    "    os.path.join(working_dir, condition_folder_partAm), \n",
    "    os.path.join(working_dir, condition_folder_partBm), \n",
    "    os.path.join(working_dir, merged_conditions), \n",
    "    conditionpartA, \n",
    "    conditionpartB\n",
    ")\n",
    "\n",
    "print(\"Die Verarbeitung, Zusammenführung und Sortierung der Dateien ist abgeschlossen.\")\n",
    "print(\"Finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee27709-8f84-4193-a476-b8f9efd4ef18",
   "metadata": {},
   "source": [
    "# Part 7 - SORTING the metabolites according to the TAXA and separately the flux, flux*abundance and abundance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4aef57-1bb8-4746-9fb0-13433571b1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ordnerpfad erstellen\n",
    "merged_folder_path = os.path.join(working_dir, merged_conditions)\n",
    "\n",
    "# Dateien durchsuchen und Liste erstellen\n",
    "file_list_csv = []\n",
    "mIDs_list = []\n",
    "\n",
    "for filename in os.listdir(merged_folder_path):\n",
    "    if filename.endswith(\"_fluxsummerge.csv\"):\n",
    "        file_list_csv.append(filename)\n",
    "        metabolite_id = filename[len(\"mIDs_\"):-len(\"_fluxsummerge.csv\")]\n",
    "        mIDs_list.append(metabolite_id)\n",
    "\n",
    "# Listen alphabetisch sortieren\n",
    "file_list_csv.sort()\n",
    "mIDs_list.sort()\n",
    "\n",
    "# mIDs_list als DataFrame speichern\n",
    "mIDs_df = pd.DataFrame(mIDs_list, columns=[\"metabolite\"])\n",
    "mIDs_df.to_csv(os.path.join(working_dir, \"mIDs_list.csv\"), index=False, sep=\";\")\n",
    "\n",
    "# Taxon Liste erstellen\n",
    "taxon_set = set()\n",
    "for filename in file_list_csv:\n",
    "    df = pd.read_csv(os.path.join(merged_folder_path, filename), sep=\";\")\n",
    "    taxon_set.update(df[\"taxon\"].unique())\n",
    "\n",
    "# Taxon Liste sortieren und als DataFrame speichern\n",
    "taxon_list = sorted(taxon_set)\n",
    "taxon_df = pd.DataFrame(taxon_list, columns=[\"taxon\"])\n",
    "taxon_df.to_csv(os.path.join(working_dir, \"taxon_list.csv\"), index=False, sep=\";\")\n",
    "\n",
    "# Leere DataFrames für die neuen CSVs erstellen\n",
    "genus_flux_metabolites_flux_df = pd.DataFrame({\"taxon\": taxon_list})\n",
    "genus_flux_metabolites_abundance_df = pd.DataFrame({\"taxon\": taxon_list})\n",
    "genus_flux_metabolites_abundflux_df = pd.DataFrame({\"taxon\": taxon_list})\n",
    "\n",
    "# Dateien durchlaufen und Daten in die neuen DataFrames übertragen\n",
    "for filename in file_list_csv:\n",
    "    metabolite_id = filename[len(\"mIDs_\"):-len(\"_fluxsummerge.csv\")]\n",
    "    df = pd.read_csv(os.path.join(merged_folder_path, filename), sep=\";\")\n",
    "    \n",
    "    # Übertragen der flux_abundance Daten\n",
    "    genus_flux_metabolites_abundflux_df = pd.merge(\n",
    "        genus_flux_metabolites_abundflux_df, \n",
    "        df[[\"taxon\", f\"flux_abundance_sum_norm_{conditionpartB}\", f\"flux_abundance_sum_norm_{conditionpartA}\"]].rename(\n",
    "            columns={\n",
    "                f\"flux_abundance_sum_norm_{conditionpartB}\": f\"flux_abundance_sum_norm_{conditionpartB}_{metabolite_id}\",\n",
    "                f\"flux_abundance_sum_norm_{conditionpartA}\": f\"flux_abundance_sum_norm_{conditionpartA}_{metabolite_id}\"\n",
    "            }\n",
    "        ), \n",
    "        on=\"taxon\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Übertragen der flux Daten\n",
    "    genus_flux_metabolites_flux_df = pd.merge(\n",
    "        genus_flux_metabolites_flux_df, \n",
    "        df[[\"taxon\", f\"flux_sum_norm_{conditionpartB}\", f\"flux_sum_norm_{conditionpartA}\"]].rename(\n",
    "            columns={\n",
    "                f\"flux_sum_norm_{conditionpartB}\": f\"flux_sum_norm_{conditionpartB}_{metabolite_id}\",\n",
    "                f\"flux_sum_norm_{conditionpartA}\": f\"flux_sum_norm_{conditionpartA}_{metabolite_id}\"\n",
    "            }\n",
    "        ), \n",
    "        on=\"taxon\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Übertragen der abundance Daten\n",
    "    genus_flux_metabolites_abundance_df = pd.merge(\n",
    "        genus_flux_metabolites_abundance_df, \n",
    "        df[[\"taxon\", f\"abundance_sum_norm_{conditionpartB}\", f\"abundance_sum_norm_{conditionpartA}\"]].rename(\n",
    "            columns={\n",
    "                f\"abundance_sum_norm_{conditionpartB}\": f\"abundance_sum_norm_{conditionpartB}_{metabolite_id}\",\n",
    "                f\"abundance_sum_norm_{conditionpartA}\": f\"abundance_sum_norm_{conditionpartA}_{metabolite_id}\"\n",
    "            }\n",
    "        ), \n",
    "        on=\"taxon\", how=\"left\"\n",
    "    )\n",
    "\n",
    "# Sortieren der DataFrames nach taxon\n",
    "genus_flux_metabolites_flux_df = genus_flux_metabolites_flux_df.sort_values(by=\"taxon\")\n",
    "genus_flux_metabolites_abundance_df = genus_flux_metabolites_abundance_df.sort_values(by=\"taxon\")\n",
    "genus_flux_metabolites_abundflux_df = genus_flux_metabolites_abundflux_df.sort_values(by=\"taxon\")\n",
    "\n",
    "# Speichern der DataFrames als CSVs\n",
    "genus_flux_metabolites_flux_df.to_csv(os.path.join(working_dir, f\"{genus_flux_metabolites_var}_flux.csv\"), index=False, sep=\";\")\n",
    "genus_flux_metabolites_abundance_df.to_csv(os.path.join(working_dir, f\"{genus_flux_metabolites_var}_abundance.csv\"), index=False, sep=\";\")\n",
    "genus_flux_metabolites_abundflux_df.to_csv(os.path.join(working_dir, f\"{genus_flux_metabolites_var}_abundflux.csv\"), index=False, sep=\";\")\n",
    "print(\"Finished. TK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbcb000-5efd-4de9-8df5-1e481d98134d",
   "metadata": {},
   "source": [
    "# Part 8 - re-checking conditions are correctly attributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb0c4e-6e25-42e2-a6f3-a3daae0c0532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, make sure that the sick and healthy conditions are in the correct order\n",
    "\n",
    "# Remember, you determined sick condition to be A or B, and control to be the other one:\n",
    "\n",
    "print(\"sick condition is:\", sick_cond)\n",
    "print(\"control coondition is:\", control_cond)\n",
    "\n",
    "#Check again that this is correct.\n",
    "\n",
    "# The difference of SICK (sick_cond) MINUS CONTROL (control_cond) will be calculated.\n",
    "# A +POSITIVE value in _diff.csv and _diffreduced.csv means higher flux in DISEASE\n",
    "# A -NEGATIVE value in _diff.csv and _diffreduced.csv means a reduced flux in DISEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3dd7b-770f-43ba-855d-4857faf4ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a difference of cachexia MINUS control.\n",
    "# new difference, if POSITIVE means elevated in disease\n",
    "#                 if negative means reduced in disease\n",
    "\n",
    "# Example \n",
    "# Input: E123_lateCHXvCTR_M754_MGdiet_v02_t10_genusfluxmeta_abundflux.csv\n",
    "# taxon\tflux_abundance_sum_norm_d12d13_chx_1_2_Propanediol_MNXM1118\tflux_abundance_sum_norm_d12d13_ctrlmca_1_2_Propanediol_MNXM1118\n",
    "#Acetatifactor\t0.150308456\t0.064411408\n",
    "#Akkermansia\t0.225251359\t0.143899104\n",
    "\n",
    "# Output ONE, contains more information, E123_lateCHXvCTR_M754_MGdiet_v02_t10_genusfluxmeta_abundflux_diff.csv\n",
    "#taxon\tflux_abundance_sum_norm_d12d13_chx_minus_d12d13_ctrlmca_1_2_Propanediol_MNXM1118\n",
    "#Acetatifactor\t0.085897048\n",
    "#Akkermansia\t0.081352255\n",
    "\n",
    "# Output TWO, use directly for R and heatmap. E123_lateCHXvCTR_M754_MGdiet_v02_t10_genusfluxmeta_abundflux_diffreduced.csv\n",
    "#metabolite\t1_2_Propanediol\n",
    "#Acetatifactor\t0.085897048\n",
    "#Akkermansia\t0.081352255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4556d0e-5e03-4b36-99f9-b638f54e0c70",
   "metadata": {},
   "source": [
    "# Part 9A - Creating flux\\*abundance files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3483b4a-5a66-40dc-a264-97926c4efbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_csv(genus_flux_metabolites_var, sick_cond, control_cond, working_dir):\n",
    "    # Load the CSV\n",
    "    file_path = os.path.join(working_dir, f\"{genus_flux_metabolites_var}_abundflux.csv\")\n",
    "    df = pd.read_csv(file_path, delimiter=';')\n",
    "    \n",
    "    # Handle empty fields by filling them with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Create a new DataFrame for the difference\n",
    "    diff_df = df[['taxon']].copy()\n",
    "\n",
    "    # Create a list to store the new columns for improved performance\n",
    "    new_columns = {}\n",
    "\n",
    "    # Process each pair of columns\n",
    "    for col in df.columns[1:]:\n",
    "        if sick_cond in col:\n",
    "            try:\n",
    "                if \"_MNX\" in col:\n",
    "                    metabolite_name = col.split(f\"{sick_cond}_\")[1].rsplit(\"_MNX\", 1)[0]\n",
    "                    metabolite_id = col.split(\"_MNX\")[1]\n",
    "                    control_col = f\"flux_abundance_sum_norm_{control_cond}_{metabolite_name}_MNX{metabolite_id}\"\n",
    "                else:\n",
    "                    metabolite_name = col.split(f\"{sick_cond}_\")[1]\n",
    "                    control_col = f\"flux_abundance_sum_norm_{control_cond}_{metabolite_name}\"\n",
    "                \n",
    "                if control_col in df.columns:\n",
    "                    new_col_name = f\"flux_abundance_sum_norm_{sick_cond}_minus_{control_cond}_{metabolite_name}\"\n",
    "                    if \"_MNX\" in col:\n",
    "                        new_col_name += f\"_MNX{metabolite_id}\"\n",
    "                    \n",
    "                    new_columns[new_col_name] = df[col] - df[control_col]\n",
    "                else:\n",
    "                    print(f\"Control column not found for: {col}\")\n",
    "            except IndexError:\n",
    "                print(f\"Error processing column: {col}\")\n",
    "\n",
    "    # Add new columns to the DataFrame at once\n",
    "    for col_name, col_data in new_columns.items():\n",
    "        diff_df[col_name] = col_data\n",
    "\n",
    "    # Save the difference DataFrame to CSV\n",
    "    diff_file_path = os.path.join(working_dir, f\"{genus_flux_metabolites_var}_abundflux_diff.csv\")\n",
    "    diff_df.to_csv(diff_file_path, sep=';', index=False)\n",
    "\n",
    "    # Create the reduced CSV with only metabolite names in the headers\n",
    "    reduced_df = diff_df.copy()\n",
    "    reduced_df.columns = ['metabolite' if col == 'taxon' else col.split(f\"{sick_cond}_minus_{control_cond}_\")[1].rsplit(\"_MNX\", 1)[0] for col in reduced_df.columns]\n",
    "    \n",
    "    # Save the reduced DataFrame to CSV\n",
    "    reduced_file_path = os.path.join(working_dir, f\"{genus_flux_metabolites_var}_abundflux_diffreduced.csv\")\n",
    "    reduced_df.to_csv(reduced_file_path, sep=';', index=False)\n",
    "\n",
    "\n",
    "process_csv(genus_flux_metabolites_var, sick_cond, control_cond, working_dir)\n",
    "\n",
    "print(\"Finished. TK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f7022-02f9-437a-8886-53b7c6a46991",
   "metadata": {},
   "source": [
    "# Part 9B - Creating abundance files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb1ac6-9a73-4f8f-8f7f-32aa367183f1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_csv2(genus_flux_metabolites_var, sick_cond, control_cond, working_dir):\n",
    "    # Load the CSV\n",
    "    file_path = os.path.join(working_dir, f\"{genus_flux_metabolites_var}_abundance.csv\")\n",
    "    df = pd.read_csv(file_path, delimiter=';')\n",
    "    \n",
    "    # Handle empty fields by filling them with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Create a new DataFrame for the difference\n",
    "    diff_df = df[['taxon']].copy()\n",
    "\n",
    "    # Create a list to store the new columns for improved performance\n",
    "    new_columns = {}\n",
    "\n",
    "    # Process each pair of columns\n",
    "    for col in df.columns[1:]:\n",
    "        if sick_cond in col:\n",
    "            try:\n",
    "                if \"_MNX\" in col:\n",
    "                    metabolite_name = col.split(f\"{sick_cond}_\")[1].rsplit(\"_MNX\", 1)[0]\n",
    "                    metabolite_id = col.split(\"_MNX\")[1]\n",
    "                    control_col = f\"abundance_sum_norm_{control_cond}_{metabolite_name}_MNX{metabolite_id}\"\n",
    "                else:\n",
    "                    metabolite_name = col.split(f\"{sick_cond}_\")[1]\n",
    "                    control_col = f\"abundance_sum_norm_{control_cond}_{metabolite_name}\"\n",
    "                \n",
    "                if control_col in df.columns:\n",
    "                    new_col_name = f\"abundance_sum_norm_{sick_cond}_minus_{control_cond}_{metabolite_name}\"\n",
    "                    if \"_MNX\" in col:\n",
    "                        new_col_name += f\"_MNX{metabolite_id}\"\n",
    "                    \n",
    "                    new_columns[new_col_name] = df[col] - df[control_col]\n",
    "                else:\n",
    "                    print(f\"Control column not found for: {col}\")\n",
    "            except IndexError:\n",
    "                print(f\"Error processing column: {col}\")\n",
    "\n",
    "    # Add new columns to the DataFrame at once\n",
    "    for col_name, col_data in new_columns.items():\n",
    "        diff_df[col_name] = col_data\n",
    "\n",
    "    # Save the difference DataFrame to CSV\n",
    "    diff_file_path = os.path.join(working_dir, f\"{genus_flux_metabolites_var}_abundance_diff.csv\")\n",
    "    diff_df.to_csv(diff_file_path, sep=';', index=False)\n",
    "\n",
    "    # Create the reduced CSV with only metabolite names in the headers\n",
    "    reduced_df = diff_df.copy()\n",
    "    reduced_df.columns = ['metabolite' if col == 'taxon' else col.split(f\"{sick_cond}_minus_{control_cond}_\")[1].rsplit(\"_MNX\", 1)[0] for col in reduced_df.columns]\n",
    "    \n",
    "    # Save the reduced DataFrame to CSV\n",
    "    reduced_file_path = os.path.join(working_dir, f\"{genus_flux_metabolites_var}_abundance_diffreduced.csv\")\n",
    "    reduced_df.to_csv(reduced_file_path, sep=';', index=False)\n",
    "\n",
    "\n",
    "process_csv2(genus_flux_metabolites_var, sick_cond, control_cond, working_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec9606-6aac-4415-8d40-463288f0b7e6",
   "metadata": {},
   "source": [
    "# Part 9C - Creating flux files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff199e8-a438-4e00-87db-f5c67cbc0224",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_csv3(genus_flux_metabolites_var, sick_cond, control_cond, working_dir):\n",
    "    # Load the CSV\n",
    "    file_path = os.path.join(working_dir, f\"{genus_flux_metabolites_var}_flux.csv\")\n",
    "    df = pd.read_csv(file_path, delimiter=';')\n",
    "    \n",
    "    # Handle empty fields by filling them with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Create a new DataFrame for the difference\n",
    "    diff_df = df[['taxon']].copy()\n",
    "\n",
    "    # Create a list to store the new columns for improved performance\n",
    "    new_columns = {}\n",
    "\n",
    "    # Process each pair of columns\n",
    "    for col in df.columns[1:]:\n",
    "        if sick_cond in col:\n",
    "            try:\n",
    "                if \"_MNX\" in col:\n",
    "                    metabolite_name = col.split(f\"{sick_cond}_\")[1].rsplit(\"_MNX\", 1)[0]\n",
    "                    metabolite_id = col.split(\"_MNX\")[1]\n",
    "                    control_col = f\"flux_sum_norm_{control_cond}_{metabolite_name}_MNX{metabolite_id}\"\n",
    "                else:\n",
    "                    metabolite_name = col.split(f\"{sick_cond}_\")[1]\n",
    "                    control_col = f\"flux_sum_norm_{control_cond}_{metabolite_name}\"\n",
    "                \n",
    "                if control_col in df.columns:\n",
    "                    new_col_name = f\"flux_sum_norm_{sick_cond}_minus_{control_cond}_{metabolite_name}\"\n",
    "                    if \"_MNX\" in col:\n",
    "                        new_col_name += f\"_MNX{metabolite_id}\"\n",
    "                    \n",
    "                    new_columns[new_col_name] = df[col] - df[control_col]\n",
    "                else:\n",
    "                    print(f\"Control column not found for: {col}\")\n",
    "            except IndexError:\n",
    "                print(f\"Error processing column: {col}\")\n",
    "\n",
    "    # Add new columns to the DataFrame at once\n",
    "    for col_name, col_data in new_columns.items():\n",
    "        diff_df[col_name] = col_data\n",
    "\n",
    "    # Save the difference DataFrame to CSV\n",
    "    diff_file_path = os.path.join(working_dir, f\"{genus_flux_metabolites_var}_flux_diff.csv\")\n",
    "    diff_df.to_csv(diff_file_path, sep=';', index=False)\n",
    "\n",
    "    # Create the reduced CSV with only metabolite names in the headers\n",
    "    reduced_df = diff_df.copy()\n",
    "    reduced_df.columns = ['metabolite' if col == 'taxon' else col.split(f\"{sick_cond}_minus_{control_cond}_\")[1].rsplit(\"_MNX\", 1)[0] for col in reduced_df.columns]\n",
    "    \n",
    "    # Save the reduced DataFrame to CSV\n",
    "    reduced_file_path = os.path.join(working_dir, f\"{genus_flux_metabolites_var}_flux_diffreduced.csv\")\n",
    "    reduced_df.to_csv(reduced_file_path, sep=';', index=False)\n",
    "\n",
    "# Example usage\n",
    "#genus_flux_metabolites_var = 'example_genus_flux_metabolites_var'\n",
    "#sick_cond = 'sick_cond'\n",
    "#control_cond = 'control_cond'\n",
    "#working_dir = 'path/to/working_dir'\n",
    "process_csv3(genus_flux_metabolites_var, sick_cond, control_cond, working_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e4be0-797d-438e-a52a-533e7034bd25",
   "metadata": {},
   "source": [
    "# Script Search flux species\n",
    "\n",
    "Finished!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qiime2-2024.2_ampl-Micom0.35.0",
   "language": "python",
   "name": "qiime2-2024.2_ampl-micom0.35.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
